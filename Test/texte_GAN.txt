Abstract
Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by hu- mans) from real images. Most images so gen- erated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to intro- duce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be trans- ferred to facilitate generation in a perceptually- distinct target domain with limited training data. To further adapt the transferred filters to the tar- get domain, we propose adaptive filter modula- tion (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.

1. Introduction
Recent research has demonstrated the increasing power of generative adversarial networks (GANs) to generate high-quality samples, that are often indistinguishable from real data (Karras et al., 2017; Lucic et al., 2018; Miyato et al., 2018; Brock et al., 2019; Karras et al., 2019a); this demonstrates the capability of GANs to exploit the valu- able information within the underlying data distribution.
*Equal contribution 1Department of Electrical and Computer Engineering, Duke University, Durham NC, USA. Correspondence to: Miaoyun Zhao <miaoyun9zhao@gmail.com>, Yulai Cong <yulaicong@gmail.com>.
Proceedings of the 37th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).
Although many powerful GAN models pretrained on large- scale datasets have been released, few efforts have been made (Giacomello et al., 2019) to take advantage of the valuable information within those models to facilitate down- stream tasks; this shows a clear contrast with the popularity of transfer learning for discriminative tasks (e.g., to reuse the feature extractor of a pretrained classifier) (Bengio, 2012; Donahue et al., 2014; Luo et al., 2017; Zamir et al., 2018), and transfer learning in natural language processing (e.g., to reuse the expensively-pretrained BERT model) (Devlin et al., 2018; Bao & Qiao, 2019; Peng et al., 2019; Mozafari et al., 2019).
Motivated by the significant value of released pretrained GAN models, we propose to leverage the information therein to facilitate downstream tasks in a target domain with limited training data. This situation arises frequently due to expensive data collection or privacy issues that may arise in medical or biological applications (Yi et al., 2019). We con- centrate on the challenging scenario of GAN model develop- ment when limited training data are available. One key ob- servation motivating our method is that a well-trained GAN can generate realistic images not observed in the training dataset (Brock et al., 2019; Karras et al., 2019a; Han et al., 2019), demonstrating the generalization ability of GANs to capture the training data manifold. Likely arising from novel combinations of information/attributes/styles (see stunning illustrations in StyleGAN (Karras et al., 2019a)), this gen- eralization of GANs is extremely appealing for scenarios in which there are limited data (Yi et al., 2019; Han et al., 2019). For example, GANs can be used to augment the training set via realistic data generation, to alleviate overfit- ting or provide regularizations for classification (Wang & Perez, 2017; Frid-Adar et al., 2018), segmentation (Bowles et al., 2018), or detection (Han et al., 2019; 2020).
However, the limited data in the target domain manifests a problem in learning the underlying GAN model, as GANs typically require substantial training data. When a limited quantity of data are available, to naively train a GAN is prone to overfitting, as powerful GAN models have numer- ous parameters that are essential for realistic generation (Bermudez et al., 2018; Bowles et al., 2018; Frid-Adar et al., 2018; Finlayson et al., 2018). To alleviate overfit- ting, one may consider transferring additional information from other domains via transfer learning, which may deliver
simultaneously better training efficiency and performance (Caruana, 1995; Bengio, 2012; Sermanet et al., 2013; Don- ahue et al., 2014; Zeiler & Fergus, 2014; Girshick et al., 2014). However, most transfer learning work has focused on discriminative tasks, based on the foundation that low- level filters (those close to input observations) of a classifier pretrained on a large-scale source dataset are fairly gen- eral (like Gabor filters) and thus transferable to different target domains (Yosinski et al., 2014); as the well-trained low-level filters (often data-demanding (Fre ́gier & Gouray, 2019; Noguchi & Harada, 2019)) provide additional infor- mation, transfer learning often leads to better performance (Yosinski et al., 2014; Long et al., 2015; Noguchi & Harada, 2019). Compared to transfer learning on discriminative tasks, fewer efforts have been made for generation tasks (Shin et al., 2016b; Wang et al., 2018b; Noguchi & Harada, 2019), as summarized in Section 2. The work presented here addresses this challenge, considering transfer learning for GANs when there are limited data in the target domain.
Leveraging insights from the aforementioned transfer learn- ing on discriminative tasks, we posit that the low-level filters of a GAN discriminator pretrained on a large-scale source dataset are likely to be generalizable and hence transferable to various target domains. For a pretrained GAN generator, it’s shown (Bau et al., 2017; 2018; Karras et al., 2019a) that the low-level layers (those close to output observations) capture properties of generally-applicable local patterns like materials, edges, and colors, while the high-level layers (those distant from observations) are associated with more domain-specific semantic aspects of data. We therefore con- sider transferring/freezing the low-level filters from both the generator and discriminator of a pretrained GAN model to facilitate generation in perceptually-distinct target domains with limited training data. As an illustrative example, we consider the widely studied GAN scenario of natural-image generation, although the proposed techniques are general and may be applicable to other domains, such as in medicine or biology. The principal contributions of this paper are as follows.
• We demonstrate empirically that the low-level filters (within both the generator and the discriminator) of a GAN model, pretrained on large-scale datasets, can be transferred to perceptually-distinct target domains, yield- ing improved GAN performance in scenarios for which limited training data are available.
• We tailor a compact domain-specific network to harmo- niously cooperate with the transferred low-level filters, which enables style mixing for diverse synthesis.
• To better adapt the transferred filters to the target do- main, we introduce an approach we term adaptive filter modulation (AdaFM), that delivers boosted performance.
• Extensive experiments are conducted to verify the effec- tiveness of the proposed techniques.
2. Background and Related Work
2.1. Generative Adversarial Networks (GANs)
GANs have demonstrated increasing power for synthesiz- ing highly realistic data (Brock et al., 2019; Karras et al., 2019a;b); accordingly, they are widely applied in various research fields, such as image (Hoffman et al., 2017; Ledig et al., 2017; Ak et al., 2019), text (Lin et al., 2017; Fedus et al., 2018; Wang & Wan, 2018; Wang et al., 2019), video (Mathieu et al., 2015; Wang et al., 2017; 2018a; Chan et al., 2019), and audio (Engel et al., 2019; Yamamoto et al., 2019; Kumar et al., 2019).
A GAN often consists of two adversarial components, i.e., a generator G and a discriminator D. As the adversarial game proceeds, the generator learns to synthesize increasingly realistic fake data, to confuse the discriminator; the discrim- inator seeks to discriminate real and fake data synthesized by the generator. The standard GAN objective (Goodfellow et al., 2014) is
2.2. GANs on Limited Data
Existing work addressing the design of GANs based on limited data can be roughly summarized into two groups.
Exploit GANs for better usage of the information within the limited data. In addition to traditional data augmenta- tions like shift, zooming, rotation, or flipping, GANs trained on limited data can be leveraged for synthetic augmentations, like synthesized observations with transformed styles (Wang & Perez, 2017) or fake observation-label/segmentation pairs (Bowles et al., 2018; Frid-Adar et al., 2018; Han et al., 2019; 2020). However, because of the limited available data, a relatively small GAN model is often employed, leading to reduced generative power. Furthermore, only the informa- tion within the limited data are utilized.
Use GANs to transfer additional information to facili- tate generation with limited data. As the available data are limited, it’s often preferred to transfer additional infor- mation from other domains via transfer learning (Yosinski et al., 2014; Long et al., 2015; Noguchi & Harada, 2019). TransferGAN (Wang et al., 2018b) initializes the target GAN model with parameters pretrained on source large- scale datasets, followed by fine-tuning the whole model with the limited target data. As source model architecture (often large) is directly transferred to the target domain, fine-tuning with too limited target data may suffer from overfitting, as verified empirically in our experiments; since
the high-level semantically specific filters are also trans- ferred, the similarity between the source and target domains is often critical for a beneficial transfer (Wang et al., 2018b). Similarly, based on the assumption that the source and tar- get domains share the same support, Wang et al. (2020) introduces an additional miner network to mine knowledge from pretrained GANs to form target generation, likewise fine-tuning the whole model with the limited data.
Different from the above fine-tuning methods, Noguchi & Harada (2019) propose batch-statistics adaptation (BSA) to transfer/freeze the whole source generator but introduce new trainable parameters to adapt its hidden batch statistics for generation with extremely-limited target data; however, the generator is not adversarially trained (L1/Perceptual loss is used instead), leading to blurry generation in the target domain (Noguchi & Harada, 2019). By compari- son, our method transfers/freezes the generally-applicable low-level filters – in the generator and discriminator – from source to (perceptually-distinct) target domains, followed by employing a small tailored high-level network and the newly-introduced adaptive filter modulation (AdaFM) to better adapt to the target limited data. Accordingly, our
proposed method, when compared to the fine-tuning meth- ods (Wang et al., 2018b; 2020), is expected to suffer less from overfitting and behave more robustly in the presence of differences between the source and target domains; when compared to (Noguchi & Harada, 2019), our method is more flexible and provides clearly better generation, thanks to its adversarial training.
Recently, a concurrent work (Mo et al., 2020) reveals that freezing low-level layers of the GAN discriminator delivers better fine-tuning of GANs; it may be viewed as a special case of our method, which transfers/freezes low-level layers of both the generator and discriminator, leveraging a tailored high-level network and employing the proposed AdaFM. Consequently, our method is expected to perform better on (extremely) limited data.
3. Proposed Method
For GAN training with limited data in the target domain, we propose to transfer additional information by leverag- ing the valuable low-level filters (those close to observa- tions) from existing GANs pretrained on large-scale source
datasets. Combining that prior knowledge, from the trans- ferred low-level filters that are often generally-applicable but data-demanding to train (Yosinski et al., 2014; Fre ́gier & Gouray, 2019), one may expect less overfitting and hence better GAN performance. Specifically, given a pretrained GAN model, we reuse its low-level filters (termed the gen- eralizable or general part of the model) in a target domain, and replace the high-level layers (termed the domain spe- cific part) with another smaller network, and then train that specific part using the limited target data, while keeping the transferred general part frozen (see Figures 1(f) and (g)). Hence, via this approach, we leverage the transferred general part, trained on a much larger source dataset, and by employing the simplified domain-specific part, the to- tal number of parameters that need be learned is reduced substantially, aligning with the limited target-domain data.
In what follows, we take natural image generation as an ex- ample, and present our method by answering three questions in Sections 3.1, 3.2, and 3.3, respectively:
• How to specify the general part appropriate for transfer? • How to tailor the specific part so that it is simplified?
• How to better adapt the transferred general part?
Before introducing the proposed techniques in detail, we first discuss source datasets, available pretrained GAN mod- els, and evaluation metrics. Intuitively, to realize generally- applicable low-level filters, one desires a large-scale source dataset with rich diversity. In the context of image analysis, a common choice is the ImageNet dataset (Krizhevsky et al., 2012; Shin et al., 2016a), which contains 1.2 million high- resolution images from 1000 classes; we consider this as the source dataset. Concerning publicly available GAN models pretrained on ImageNet, available choices include SNGAN (Miyato et al., 2018), GP-GAN (Mescheder et al., 2018), and BigGAN (Brock et al., 2019); we select the pretrained GP-GAN model (with resolution 128 × 128) because of its well-written codebase. To evaluate the generative perfor- mance, we adopt the widely used Fre ́chet inception distance (FID, lower is better) (Heusel et al., 2017), a metric assess- ing the realism and variation of generated samples (Zhang et al., 2018).
3.1. On Specifying the General Part for Transfer
As mentioned in the Introduction, both generative and dis- criminative image models share a similar pattern: higher- level convolutional filters portray more domain-specific se- mantic information, while lower-level filters portray more generally applicable information (Yosinski et al., 2014; Zeiler & Fergus, 2014; Bau et al., 2017; 2018). Given the GP-GAN model pretrained on ImageNet, the question is how to specify the low-level filters (i.e., the general part of the model) to be transferred to a target domain. Generally speaking, the optimal solution is likely to be a compromise depending on the available target-domain data; if plenty
Figure 2. Sample images from the ImageNet and CelebA datasets. Although quite different, they are likely to share the same set of low-level filters describing basic shapes, like lines, curves and textures.
of data are provided, less low-level filters should be trans- ferred (less prior knowledge need be transferred), but when the target-domain data are limited, it’s better to transfer more filters (leveraging more prior information). We empir- ically address that question by transferring the pretrained GP-GAN model to the CelebA dataset (Liu et al., 2015), which is fairly different from the source ImageNet (see Figure 2). It’s worth emphasizing that the general part dis- covered here1 also delivers excellent results on three other datasets (see the experiments and Appendix E).
3.1.1. GENERAL PART OF THE GENERATOR
To determine the appropriate general part of the GP-GAN generator, to be transferred to the target CelebA dataset,2 we employ the GP-GAN architecture and design experiments with increasing number of lower layers included in the trans- ferred/frozen general part of the generator; the remaining specific part (not transferred) of the generator (see Figure 1(f)) and the discriminator are reinitialized and trained with CelebA.
Four settings for the generator general part are tested, i.e., 2, 4, 5, and 6 lower groups to be transferred (termed G2, G4, G5, and G6, respectively; G4 is illustrated in Figure 1(f)). After 60,000 training iterations (generative quality stabilizes by then), and we show in Figure 3 the generated samples and FIDs of the four settings. It’s clear that transferring the G2/G4 generator general part delivers decent generative quality (see eye details, hair texture, and cheek smoothness), despite the fact that the source ImageNet is perceptually distant from the target CelebA, confirming the generalizable
1 This general part may not be the optimum with the best generalization, which is deemed intractable. The key is that it’s applicable to various target domains (see the experiments); in addition, the AdaFM technique introduced in Section 3.3 delivers significantly improved adaptation/transfer and thus greatly relaxes the requirement of an optimal selection of the general part.
2 To verify the generalization of the pretrained filters, we bypass the limited-data assumption in this section and employ the whole CelebA data for training.
nature of the low-level filters within up to 4 lower groups of
the pretrained GP-GAN generator (which is also verified on three other datasets in the experiments). The lower FID of G4 than that of G2 indicates that transferring more low-level filters pretrained on large-scale source datasets potentially benefits better performance in target domains.3 But when we transfer and hence freeze more groups as the general part of the generator (i.e., G5 and G6), the generative quality drops quickly; this is expected as higher-level filters are more specific to the source ImageNet and may not fit the target CelebA. By reviewing Figure 3, we choose G4 as the setting for the generator general part for transfer.
3.1.2. GENERAL PART OF THE DISCRIMINATOR
Based on the G4 general part of the generator, we next conduct experiments to specify the general part of the dis- criminator. We consider transferring/freezing 0, 2, 3, and 4 lower groups of the pretrained GP-GAN discriminator (termed D0, D2, D3, and D4, respectively; D2 is illustrated in Figure 15 of the Appendix). Figure 4 shows the generated samples and FIDs for each setting. Similar to what’s ob- served for the generator, transferring low-level filters from the pretrained GP-GAN discriminator also benefits a bet- ter generative performance (compare the FID of D0 with that of D2), thanks to the additional information therein; however, as the higher-level filters are more specific to the source ImageNet, transferring them may lead to a decreased generative quality (see the results from D3 and D4).
Considering both the generator and discriminator, we trans- fer/freeze the G4D2 general part4 from the pretrained GP-
3 Another reason might be that to train well-behaved low-level filters is time-consuming and data-demanding (Fre ́gier & Gouray, 2019; Noguchi & Harada, 2019). The worse FID of G2 is believed caused by the insufficiently trained low-level filters, as we find the images from G2 show a relatively lower diversity and contain strange textures in the details (see Figure 18 in Appendix). FID is biased toward texture rather than shape (Karras et al., 2019b).
4 Appendix C.2 discusses other settings for the general part.
Figure 5. Generated images from the GPHead and SmallHead trained on the Flowers dataset (Nilsback & Zisserman, 2008).
GAN model, which will be shown in the experiments to work quite well on three other target datasets.
3.2. On Tailoring the High-Level Specific Part
Even with the transferred/frozen G4D2 general part, the remaining specific (target-dependent) part may contain too many trainable parameters for the limited target-domain data (e.g., the GPHead model in Figure 1(f) shows mode collapse (see Figure 5) when trained on the small Flowers dataset (Nilsback & Zisserman, 2008)); another considera- tion is that, when using GANs for synthetic augmentation for applications with limited data, style mixing is a highly appealing capability (Wang & Perez, 2017). Motivated by those considerations, we propose to replace the high-level specific part of GP-GAN with a tailored smaller network (see Figure 1(g)), to alleviate overfitting, enable style mix- ing, and also lower the computational/memory cost.
Specifically, that tailored specific part is constructed as a fully connected (FC) layer followed by two successive style blocks (borrowed from StyleGAN (Karras et al., 2019a) with an additional short cut, see Figure 1(c)). Similar to Style- GAN, the style blocks enable unsupervised disentanglement of high-level attributes, which may benefit an efficient explo- ration of the underlying data manifold and thus lead to better generation; they also enable generating samples with new attribute combinations (style mixing), which dramatically enlarges the generation diversity (see Figures 14 and 28 of the Appendix). We term the model consisting of the tailored specific part and the G4D2 general part as SmallHead. Note
that proposed specific part is also used in our method (see Figure 1(h)). Different from the GPHead, the SmallHead has proven to train in a stable manner, without mode col- lapse on Flowers (see Figure 5). In the experiments, the SmallHead is found to work well on other small datasets.
3.3. Better Adaption of the Transferred General Part
Based on the above transferred general part and tailored spe- cific part, we next present a new technique, termed adaptive filter modulation (AdaFM), to better adapt the transferred low-level filters to target domains for boosted performance, as shown in the experiments. In this proposed approach, we no longer just “freeze” the transferred filters upon transfer, but rather augment them in a target-dependent manner.
Motivated by the style-transfer literature (Huang & Be- longie, 2017; Noguchi & Harada, 2019), where one ma- nipulates the style of an image by modifying the statistics (e.g., mean or variance) of its latent feature maps, we alter- natively consider the variant of manipulating the style of a function (i.e., the transferred general part) by modifying the statistics of its convolutional filters via AdaFM.
Specifically, given a transferred convolutional filter W ∈ RCout×Cin×K1×K2 in the general part, where Cin/Cout de- notes the number of input/output channels and K1 × K2 is the kernel size, AdaFM introduces a small amount of
Figure 7. Generated samples from our model (a) with AdaFM, and (b) with AdaFM replaced by FS. (c) FID scores along training.
AdaFM, the weight demodulation employs zero shift β = 0 and a rank-one scale γ = ηsT , where style s ∈ RCin is produced by a trainable mapping network (often a MLP) and η ∈ RCout is calculated as
η=q i
learnable parameters, i.e., scale γ ∈ RCout×Cin β ∈ RCout ×Cin , to modulate its statistics via
and shift (2)
where ε is a small constant to avoid numerical issues. De- spite being closely related, AdaFM and the weight demod- ulation are motivated differently. We propose AdaFM to better adapt the transferred filters to target domains, while the weight demodulation is used to relax instance normal- ization while keeping the capacity for controllable style mixing (Karras et al., 2019b). See Appendix C.7 for more discussions.
Another special case of AdaFM is the filter selection (FS) presented in (Noguchi & Harada, 2019), which employs rank-one simplification to both scale γ and shift β. Specif- ically, γ = γˆ1T and shift β = βˆ1T with γˆ ∈ RCout and βˆ ∈ RCout (see Figure 1(d)). The goal of FS is to “select” the transferred filters W; for example if γˆ is a binary vector, it literally selects among {Wi,:,:,:}. As FS doesn’t modulate among Cin input channels, its basic assumption is that the source and target domains share the same correlation among those channels, which might not be true. See the illustra- tive example in Figure 6, where the source/target domain has the basic pattern/filter of an almost-red/almost-green square (the shape is the same); it’s clear simply selecting (FS) the source filter won’t deliver a match to the target, ap- pealing for a modulation (AdaFM) among (input) channels. Figure 7 shows results from models with FS and AdaFM (using the same architecture in Figure 1(h)). It’s clear that AdaFM brings boosted performance, empirically support- ing the above intuition that the basic shape/pattern within each Wi,j,:,: are generally applicable while the correlation among i-,j-channels may be target-specific (this is further verified by AdaFM delivering boosted performance on other datasets in the experiments).
4. Experiments
Taking natural image generation as an illustrative example, we demonstrate the effectiveness of the proposed techniques
for i ∈ {1,2,··· ,Cout} and j ∈ {1,2,··· ,Cin}. WAdaFM is then used to convolve with input feature maps for out- put ones. Applying AdaFM to convolutional kernels of a residual block (see Figure 1(a)) (He et al., 2016) gives the AdaFM block (see Figure 1(b)). With the residual blocks of the SmallHead replaced with AdaFM blocks, we yield our generator, as shown in Figure 1(h), which delivers boosted performance than the SmallHead in the experiments.
For better understanding the power of our AdaFM, below we draw parallel connections to two related techniques, which may be viewed as the special cases of AdaFM to some extent. The first one is the weight demodulation revealed in the recent StyleGAN2 (Karras et al., 2019b), a model with state-of-the-art generative performance. Compared with
by transferring the source GP-GAN model pretrained on the large-scale ImageNet (containing 1.2 million images from 1,000 classes) to facilitate generation in perceptually-distinct target domains with (i) four smaller datasets, i.e., CelebA (Liu et al., 2015) (202,599), Flowers (Nilsback & Zisser- man, 2008) (8,189), Cars (Krause et al., 2013) (8,144), and Cathedral (Zhou et al., 2014) (7,350); (ii) their modified variants containing only 1,000 images; and (iii) two ex- tremely limited datasets consisting of 25 images (following (Noguchi & Harada, 2019)).
The experiments proceed by (i) demonstrating the advan- tage of our method over existing approaches; (ii) con- ducting ablation studies to analyze the contribution of each component of our method; (iii) verifying the pro- posed techniques in challenging settings with only 1,000 or 25 target images; (iv) analyzing why/how AdaFM leads to boosted performance; and (v) illustrating the poten- tial for exploiting the tailored specific part of our model for data augmentation for applications with limited data. Generated images and FID scores (Heusel et al., 2017) are used to evaluate the generative performance. De- tailed experimental settings and more results are provided in the Appendix. Code is available at github.com/ MiaoyunZhao/GANTransferLimitedData.
4.1. Comparisons with Existing Methods
To demonstrate our contributions over existing approaches, we compare our method with (i) TransferGAN (Wang et al., 2018b), which initializes with the pretrained GP- GAN model (accordingly the same network architecture is adopted; refer to Figure 1(f)), followed by fine-tuning all parameters on the target data. We also consider (ii) Scratch,
Table 1. FID scores of the compared methods after 60,000 training iterations. Lower is better. “Failed” means training/mode collapse.
Method\Target CelebA Flowers Cars Cathedral
TransferGAN Scratch Our
18.69 failed failed 16.51 29.65 11.77 9.90 16.76 10.10
failed 30.59 15.78
which trains a model with the same architecture as ours (see Figure 1(h)) from scratch with the target data.
The experimental results are shown in Figure 8, with the fi- nal FID scores summarized in Table 1. Since TransferGAN employs the source (large) GP-GAN architecture, it may suf- fer from overfitting if the target data are too limited, which manifests as training/mode collapse; accordingly, Trans- ferGAN fails on the 3 small datasets: Flowers, Cars, and Cathedral. By comparison, thanks to the tailored specific part, both Scratch and our method train stably on all target datasets, as shown in Figure 8. Compared to Scratch, our method shows dramatically increased training efficiency, thanks to the transferred low-level filters, and significantly improved generative quality (much better FIDs in Table 1), which are attributed to both the transferred general part and a better adaption to target domains with AdaFM.
4.2. Ablation Study of Our Method
To reveal how each component contributes to the excellent performance of our method, we consider four experimental settings in a sequential manner. (a) GP-GAN: adopt the GP- GAN architecture (similar to Figure 1(f) but all parameters are trainable and randomly initialized), used as a baseline where no low-level filters are transferred. (b) GPHead: use the model in Figure 1(f), to demonstrate the contribution
of the transferred general part. (c) SmallHead: employ the model in Figure 1(g), to reveal the contribution of the tailored specific part. (d) Our: leverage the model in Figure 1(h), to show the contribution of the presented AdaFM.
The FID curves during training and the final FID scores of the compared methods are shown in Figure 9 and Ta- ble 2, respectively. By comparing GP-GAN with GPHead on CelebA, it’s clear that the transferred general part con- tributes by dramatically increasing the training efficiency and by delivering better generative performance; this is con- sistent with what’s revealed in the previous section (com- pare Scratch with Our in Figure 8 and Table 1). Comparing SmallHead to both GPHead and GP-GAN in Table 2 indi- cates that the tailored specific part helps alleviate overfitting and accordingly delivers stable training. By better adapting the transferred general part to the target domains, the pro- posed AdaFM contributes most to the boosted performance (compare SmallHead with Our in Figure 9 and Table 2), empirically confirming our intuition in Section 3.3.
4.3. Generation with Extremely Limited Data
To verify the effectiveness of the proposed techniques in more challenging settings, we consider generation with only
1,000 or 25 target samples. Specifically, we randomly select
and Cathedral-1K, respectively. Since TransferGAN fails
when given about 8,000 target images (see Section 4.1), we omit it and only compare our method with Scratch on these 1K variants. Regarding the extremely limited setup with 25 samples, we follow Noguchi & Harada (2019) to select 25 images from Flowers and FFHQ (Karras et al., 2019a) to form the Flowers-25 and FFHQ-25 datasets, on which their BSA and our method are compared.
The FID curves versus training iterations on the 1K datasets
are shown in Figure 10, with the lowest FIDs summarized
in Table 3. In this challenging setting, both Scratch and our
method with the G4D2 general part (labeled Our-G4D2)
suffer from overfitting. Scratch suffers more due to more
trainable parameters; as our method has a much higher
training efficiency, a false impression may potentially arise
(Cong et al., 2019; Mo et al., 2020); for clarity, see the
significantly improved best performance of our method and
the more gentle ending slope of its FID curves. To allevi-
ate overfitting, we transfer more discriminator filters from
the pretrained GP-GAN model, with the results also given
in Figure 10 and Table 3. It’s clear that intuitive patterns
emerge, i.e., less data appeal for more transferred informa-
tion. On the other hand, the comparable FIDs of Our-G4D2
(see Table 3) indicates that the G4D2 general part discovered BSA
in Section 3.1 works fairly well on these 1K datasets. Con-
cerning early stopping for generation with limited data, we
for that goal in our setup (see Appendix C.5 for detailed

discussions).
On Flowers-25 and FFHQ-25, since the target data are ex- tremely limited in quantity, we transfer more filters (i.e., G4D6) from the pretrained GP-GAN model and apply GP (gradient penalty) on both real and fake samples to alleviate overfitting (see Appendix B for detailed settings). Figure 11 shows the generated samples and FID scores from the BSA (Noguchi & Harada, 2019) and our method. It’s clear that our method with the G4D6 general part works reason- ably well even in such settings with extremely limited data, with a much better performance than the L1/Perceptual-loss- based BSA. To illustrate the learned data manifold, Figure 12 shows the smooth interpolations between two random samples from our method, demonstrating the effectiveness of the proposed techniques on generation with extremely limited data.
4.4. Analysis of AdaFM and Style Augmentation with the Tailored Specific Part
To better understand why adopting AdaFM in the transferred general part of our model leads to boosted performance, we summarize in Figures 13(a) and 13(b) the learned scale γ and shift β from different groups of the generator general part. Apparently, all transferred filters are used in the target domains (no zero-valued γ) but with modulations (γ/β has values around 1/0). As AdaFM delivers boosted perfor- mance, it’s clear those modulations are crucial for a success- ful transfer from source to target domain, confirming our intuition in Section 3.3. To illustrate how γ/β behaves on different target datasets, we show in Figure 13(c) the sorted comparisons of the learned γ in Group 2; as expected, dif- ferent datasets prefer different modulations, justifying the necessity of AdaFM and its performance gain. Concerning further demonstration of AdaFM and medical/biological applications with gray-scale images, we conduct another experiment on a gray-scale variant of Cathedral (results are given in Appendix D due to space constraints); we find that without AdaFM to adapt the transferred filters, worse (blurry and messy) details are observed in the generated images (refer also to Figure 7), likely because of the mis-
Figure 14. Style mixing on Flowers via the tailored specific part of our model. The “Source” sample controls flower shape, location, and background, while the “Destination” sample controls color and petal details.
matched correlation among channels between source and target domains.
To reveal the potential in exploiting the tailored specific part of our model for data augmentation for applications with limited data, we conduct style mixing with the specific part, following (Karras et al., 2019a). Figure 14 shows the results on Flowers (see Appendix F for details and more results). It’s clear that style mixing enables synthesizing a vast set of new images via style/attribute combinations. Therefore, the tailored specific part of our model can be exploited for a diverse synthetic augmentation, which is believed extremely appealing for downstream applications with limited data.
5. Conclusions
We reveal that the valuable information (i.e., the low-level filters) within GAN models pretrained on large-scale source datasets (e.g., ImageNet) can be transferred to facilitate gen- eration in perceptually-distinct target domains with limited data; this transfer is performed on both the generator and discriminator. To alleviate overfitting due to the limited target-domain data, we employ a small specific network atop the transferred low-level filters, which enables style mixing for a diverse synthetic augmentation. To better adapt the transferred filters to target domains, we present adap- tive filter modulation (AdaFM), that delivers boosted per- formance on generation with limited data. The proposed techniques are shown to work well in challenging settings with extremely limited data (e.g., 1,000 or 25 samples).
Acknowledgements
We thank the anonymous reviewers for their constructive comments. The research was supported by part by DARPA, DOE, NIH, NSF and ONR. The Titan Xp GPU used was donated by the NVIDIA Corporation.